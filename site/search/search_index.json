{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OceanEye: Streamlined Image and Video Annotation","text":"<p>OceanEye is a robust desktop application built with C++ and the Qt framework, designed to enhance the efficiency of image and video annotation. It empowers users with tools for precise human-in-the-loop annotation and offers integrated, one-click training for computer vision models\u2014all executed locally.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Install OceanEye by following the step-by-step guide.</li> <li>Once installed, learn how to create your first project and start annotating.</li> </ol>"},{"location":"documentation/add-media/","title":"Adding Media","text":"<p>To add media, click the Add Media button located in the main project view. A file dialog will appear, allowing you to select one or more images or videos for import.</p> <p>Currently, we support the following formats:</p> <ul> <li><code>.MP4</code></li> <li><code>.MOV</code></li> <li><code>.PNG</code></li> <li><code>.JPG</code></li> </ul>"},{"location":"documentation/add-media/#behavior-of-imported-media","title":"Behavior of Imported Media","text":""},{"location":"documentation/add-media/#images","title":"Images","text":"<p>When importing images, OceanEye creates a reference link to the original files on your local filesystem.</p> <p>Warning</p> <p>If the images are moved or renamed after being imported, OceanEye will no longer be able to locate them. </p>"},{"location":"documentation/add-media/#videos","title":"Videos","text":"<p>When importing videos, OceanEye processes them by slicing the video into individual frames. These frames are extracted based on the slice interval specified in the application settings and are saved directly to the project folder.</p>"},{"location":"documentation/add-media/#automatic-detection-and-filtering","title":"Automatic Detection and Filtering","text":"<ul> <li>Detection on Import: If a detection model is loaded, OceanEye will automatically run detections on all imported images.</li> <li>Automatic Filtering: If the Automatically Filter Dead Video option is enabled in the settings, any frames or images without detections will be excluded from the project.</li> </ul>"},{"location":"documentation/edit-media/","title":"Edit Media","text":"<p>You can efficiently manage, view, and export your project's media with the Edit Media dialog. Select images from the grid to remove them individually, or clear all media from your project. </p>"},{"location":"documentation/edit-media/#removing-media","title":"Removing Media","text":"<p>When you remove media, the annotation information is deleted. </p> <ul> <li>Internal Media: When you remove media stored within the project's directory (e.g., sliced video frames), the corresponding files are permanently deleted.</li> <li>External Media: Media imported from outside the project's directory will be removed from the project but remain in their original location.</li> </ul>"},{"location":"documentation/export-annotations/","title":"Export Annotations","text":""},{"location":"documentation/export-annotations/#export-annotated-images","title":"Export Annotated Images","text":"<p>Annotated images can be exported either through the Edit Media dialog or directly from the main window.</p> <p>To export images via the Edit Media dialog, select the desired images and click on the \"Export Selected Images with Annotations\" button.</p> <p>To export an individual image, navigate to the image you wish to export and choose the <code>File &gt; Export Image</code> option from the menu.</p>"},{"location":"documentation/export-annotations/#export-annotation-data","title":"Export Annotation Data","text":"<p>Annotations can be exported via the <code>File &gt; Export Data</code> menu.</p> <p>The exported data includes the following fields, with one entry per annotation:</p> Field Name Description Frame Name Specifies the file path of the image containing the annotation. Class Denotes the category of the annotation (e.g., \"Sea Urchin\", \"Kelp\"). Confidence Represents the confidence score of the annotation, ranging from 0 to 1, (human-generated annotations have a confidence of 1). X Indicates the X-coordinate of the annotation's top-left corner, measured in pixels. Y Indicates the Y-coordinate of the annotation's top-left corner, measured in pixels. Width Defines the width of the annotation's bounding box, measured in pixels. Height Defines the height of the annotation's bounding box, measured in pixels."},{"location":"documentation/export-annotations/#supported-export-formats","title":"Supported Export Formats:","text":""},{"location":"documentation/export-annotations/#csv","title":"CSV","text":"<p>CSV is the most popular export format. It is a simple, tabular format where each row represents an annotation, and each column corresponds to a field.</p>"},{"location":"documentation/export-annotations/#coco","title":"COCO","text":"<p>COCO, or Common Objects in Context, is a popular machine-learning dataset format. It organizes annotations into a structured JSON file, including categories, bounding boxes, and image metadata.</p>"},{"location":"documentation/export-annotations/#json","title":"JSON","text":"<p>JSON is a flexible, lightweight data-interchange format. It exports annotations in a structured, hierarchical format, making it easy to integrate with custom pipelines or applications.</p>"},{"location":"documentation/export-annotations/#yaml","title":"YAML","text":"<p>YAML is a human-readable data serialization format. It provides a clean and concise way to represent annotations, similar to JSON but with a more readable syntax.</p>"},{"location":"documentation/import-annotations/","title":"Import Annotations","text":"<p>Annotations can be exported via the <code>File &gt; Import Annotations</code> menu.</p> <p>Currently, the only supported format is COCO. COCO is supported by popular dataset repositories such as Roboflow and Kaggle.</p> <p>To import annotations, images must be in the same folder as the COCO annotation file.</p>"},{"location":"documentation/model-training/","title":"Model Training","text":"<p>OceanEye uses the ONNX (Open Neural Network Exchange) format, which is a popular cross-platform format for sharing AI models.</p> <p>It is reccomended to use a CUDA-capable GPU. CUDA can be downloaded here. While training on a CPU is supported, it may result in significantly longer training times.</p> <p>Model training requires additional dependencies, which are automatically installed during the initial training process with OceanEye. An internet connection is required to install dependencies.</p> <p>It is recommended to have at least 200 annotations per class before initiating training. For optimal results, having over 1,000 annotations per class is strongly encouraged.</p>"},{"location":"documentation/model-training/#model-training-settings","title":"Model Training Settings","text":"Setting Description Model Save Location Filepath to the outputted model"},{"location":"documentation/model-training/#advanced-settings","title":"Advanced Settings","text":"Setting Description Base Model Specifies the filename of a supported Ultralytics model to be used as the starting point for training. Training Time Defines the maximum allowable duration for training. Set to 0 for unlimited training time. Max Epochs Indicates the total number of epochs for training, where each epoch represents a complete iteration over the dataset. Patience Determines the number of consecutive epochs without improvement in validation metrics before terminating the training process early. <p>Model training requires Python. An isolated Python environment is automatically created within the <code>.oceaneye</code> directory located in your home folder. Approximately 5 GB of additional dependencies will be installed during this process. Depending on your system's performance, training may take several hours or even multiple days. It is recommended to allow the process to run uninterrupted.</p>"},{"location":"documentation/project/","title":"Project","text":"<p>A project within OceanEye represents a collection of media, annotations, and models grouped together for analysis. It serves as a workspace where researchers can organize, explore, and interpret specific datasets.</p>"},{"location":"documentation/project/#navigation","title":"Navigation","text":""},{"location":"documentation/project/#mouse-controls","title":"Mouse Controls","text":"Mouse Action Functionality Left Click Select, Draw, or Edit annotation Right Click Pan image Scroll Wheel Zoom image in or out <p>Tip</p> <p>Annotations can alternatively be selected by clicking the corresponding row in the data table located beneath the image.</p>"},{"location":"documentation/project/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"Shortcut Action Left Previous image Right Next image N New annotation R Reset View Esc Cancel Drawing Annotation Space Change Selected Annotation Class Del Delete Selected Annotation"},{"location":"getting-started/developer/","title":"Installation for Developers -- Windows","text":"<p>The following is a Windows developer installation guide for OceanEye.</p>"},{"location":"getting-started/developer/#requirements","title":"Requirements","text":""},{"location":"getting-started/developer/#msys2-and-mingw-compiler","title":"Msys2 and MinGW Compiler","text":"<p>Download the msys2 installer. Once done, open the MinGW shell and run the following:</p> <p><pre><code>$ pacman -S mingw-w64-x86_64-gcc\n</code></pre> After this, put the path to the MinGW compiler on the Path environment variable. An example directory is: C:\\msys64\\mingw64\\bin</p>"},{"location":"getting-started/developer/#ninja","title":"Ninja","text":"<ol> <li>Download ninja-win.zip</li> <li>Extract it</li> <li>Put path to the folder the ninja.exe is in onto the Path environment variable</li> </ol>"},{"location":"getting-started/developer/#cmake","title":"CMake","text":"<p>Install using installer. CMake will automatically put itself into the Path environment variable, but you must make sure that its Path comes before the MinGW compiler Path. Otherwise, issues may occur.</p>"},{"location":"getting-started/developer/#vcpkg","title":"VCPKG","text":"<p><pre><code>git clone https://github.com/microsoft/vcpkg\ncd ./vcpkg\n./vcpkg-bootstrap.bat\n</code></pre> Follow this up by putting the path to the vcpkg folder in the path environment variable.</p> <p>A good introduction for vcpkg is here.</p>"},{"location":"getting-started/developer/#opencv","title":"OpenCV","text":"<p>Need to install this to get the opencv_videoio_ffmpeg4110_64.dll. Download the .exe installer and run it. It doesn't matter where you install opencv to, all we need is the dll it provides.</p>"},{"location":"getting-started/developer/#installation-and-building","title":"Installation and Building","text":"<p>Download the OceanEye repository.</p> <pre><code>git clone https://github.com/OceanEye-Project/OceanEye-QT\n</code></pre> <p>Inside the project directory, to build the project you'll need run cmake using the CMakePresets.json. This step will take a while as VCPKG has to download and compile all the libraries that OceanEye will need.</p> <pre><code>cmake --preset=default\n</code></pre> <p>After that, we link the code together to the OceanEye executable.</p> <pre><code>cmake --build build\n</code></pre> <p>After this, OceanEye is almost ready to use. lastly, we have to put the opencv ffmpeg dll into the same folder as the OceanEye executable so that OceanEye can properly detect objects in videos.</p> <p>Access the opencv_videoio_ffmpeg4110_64.dll file by going to extracted folder opencv &gt; build &gt; bin. Copy the file into OceanEye build folder.</p> <p>After this, OceanEye is ready to use!</p>"},{"location":"getting-started/developer/#tips","title":"Tips","text":"<ul> <li>Having more than one C/C++ compiler on your Path can cause Windows to try and use the wrong compiler for the project. The same goes for multiple installations of CMake.</li> <li>Keep your paths short, otherwise there will be issues building some of the packages.</li> </ul>"},{"location":"getting-started/first-project/","title":"Creating Your First Project","text":"<p>A project serves as a container for managing a collection of images, annotations, and models. It provides a structured environment to streamline the process of detection, annotation, and model training.</p> <pre><code>graph TD\n    A[Create a New Project] --&gt; B[Add Media]\n    B --&gt; C[Run Detections]\n    C --&gt; D[Annotate Media]\n    D --&gt; E[Train Models]\n    E --&gt; B\n    D --&gt; F[Export Annotations]</code></pre>"},{"location":"getting-started/first-project/#configuring-your-project","title":"Configuring Your Project","text":"<p>Configuring a project involves defining the following settings. You will be prompted to do so on creation of new projects.</p> Setting Description Load model Select what model will be used for detections. Specify detections Select which objects the model should write detections for. Model confidence How certain the model should be in the object in order to write the detection. Select video slice interval In the case of videos, how far apart should the frames be captured? Automatically filter dead video Select whether the video should save frames that do not include a written detection. <p>Info</p> <p>These settings may be changed at any time after initial configuration by navigating to the \"settings\" tab while in a project.</p> <p>Tip</p> <p>Remember to press \"Apply\" when you are done!</p>"},{"location":"getting-started/first-project/#adding-media","title":"Adding media","text":"<p>After configuration, you can begin to add media. Currently, we support the following formats:</p> <ul> <li><code>.MP4</code></li> <li><code>.MOV</code></li> <li><code>.PNG</code></li> <li><code>.JPG</code></li> </ul> <p>Info</p> <p>Media will be screened on upload, before being loaded into the project. Detection may be re-run on a per-frame basis with a custom confidence via the \"detect\" button, in the event that experimenting with confidence is desired.</p> <p>Warning</p> <p>Running \"detect\" on a single frame will overwrite existing detections.</p>"},{"location":"getting-started/first-project/#annotating-media","title":"Annotating media","text":"<p>Annotations represent instances of a particular object within some image. These may be created by either users or, eventually by a model -- once the user has a trained model, that is. </p> <p>Since we may expect some degree of errors in model detections, users may wish to correct the result of a model's work. That is to say: they may wish to add, remove or edit annotations. This functionality is available within currently active projects, either by selecting the + New Annotation option or using the shortcut: N. Annotation creation is by drag-and-drop, and the object category (\"class\") may be selected either by rotating through options via the Space key, or manually selecting the type of object in the top-left dropdown Class selection.</p> <p>If you are unhappy with a particular annotation, you may delete it with the Del or Backspace keys.</p> <p>Warning</p> <p>Additional object types may not be added after project creation.</p>"},{"location":"getting-started/first-project/#editing-media","title":"Editing Media","text":"<p>While in a project, the Edit Media button may be selected to bring up a dialogue showing all media in the project. From here, users may quickly select a variety of images for removal from the project, or clear the project entirely.</p>"},{"location":"getting-started/first-project/#model-training","title":"Model Training","text":"<p>While training is possible on-CPU, it is strongly recommended to use a CUDA-compatible GPU to ensure time-efficient training. </p> <p>Additional dependencies are installed during first training session. </p> <p>Warning</p> <p>An internet connection is required for first training session in order to download the required depencies for training.</p> <p>Users must provide a location for the model to be saved and also have the option to configure advances settings such as base model for an initial model to be used as a starting point, as well as several more parameters for fine-tuning your training session.</p>"},{"location":"getting-started/first-project/#exporting-data","title":"Exporting Data","text":"<p>Project data can be exported in several formats, from within the File menu. The options are as follows:</p> <ol> <li> <p>Export annotation data exports annotations in raw format (CSV, YAML, COCO or JSON) for further analysis.</p> </li> <li> <p>Additionally, media may be exported one-by-one with the annotations drawn-on for later human reference. Selecting this option will open a file selection dialogue where users may select the desired images to export.</p> </li> </ol> <p>Info</p> <p>Both aforementioned exporting techniques allow for filtering by class type - allowing for the minimum amount of data to be saved - as well as exporting without any filter applied.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>To get started with OceanEye, download the latest release from the official repository: OceanEye-QT Releases.</p>"},{"location":"getting-started/installation/#additional-notes","title":"Additional Notes","text":"<ul> <li>Ensure that your system meets the System Requirements before proceeding with the installation.</li> </ul>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Operating System: Windows 10 or later</li> </ul>"},{"location":"getting-started/installation/#model-training-requirements","title":"Model Training Requirements","text":"<ul> <li>GPU: Nvidia CUDA-Capable GPU</li> <li>Software: CUDA Toolkit Model training is possible with on just a CPU, but it is not reccomended.</li> </ul>"}]}